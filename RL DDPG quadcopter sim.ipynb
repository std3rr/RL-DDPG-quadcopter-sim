{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhysicsSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def C(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "\n",
    "def S(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "\n",
    "def earth_to_body_frame(ii, jj, kk):\n",
    "    # C^b_n\n",
    "    R = [[C(kk) * C(jj), C(kk) * S(jj) * S(ii) - S(kk) * C(ii), C(kk) * S(jj) * C(ii) + S(kk) * S(ii)],\n",
    "         [S(kk) * C(jj), S(kk) * S(jj) * S(ii) + C(kk) * C(ii), S(kk) * S(jj) * C(ii) - C(kk) * S(ii)],\n",
    "         [-S(jj), C(jj) * S(ii), C(jj) * C(ii)]]\n",
    "    return np.array(R)\n",
    "\n",
    "\n",
    "def body_to_earth_frame(ii, jj, kk):\n",
    "    # C^n_b\n",
    "    return np.transpose(earth_to_body_frame(ii, jj, kk))\n",
    "\n",
    "\n",
    "class PhysicsSim():\n",
    "    def __init__(self, init_pose=None, init_velocities=None, init_angle_velocities=None, runtime=5.):\n",
    "        self.init_pose = init_pose\n",
    "        self.init_velocities = init_velocities\n",
    "        self.init_angle_velocities = init_angle_velocities\n",
    "        self.runtime = runtime\n",
    "\n",
    "        self.gravity = -9.81  # m/s\n",
    "        self.rho = 1.2\n",
    "        self.mass = 0.958  # 300 g\n",
    "        self.dt = 1 / 50.0  # Timestep\n",
    "        self.C_d = 0.3\n",
    "        self.l_to_rotor = 0.4\n",
    "        self.propeller_size = 0.1\n",
    "        width, length, height = .51, .51, .235\n",
    "        self.dims = np.array([width, length, height])  # x, y, z dimensions of quadcopter\n",
    "        self.areas = np.array([length * height, width * height, width * length])\n",
    "        I_x = 1 / 12. * self.mass * (height**2 + width**2)\n",
    "        I_y = 1 / 12. * self.mass * (height**2 + length**2)  # 0.0112 was a measured value\n",
    "        I_z = 1 / 12. * self.mass * (width**2 + length**2)\n",
    "        self.moments_of_inertia = np.array([I_x, I_y, I_z])  # moments of inertia\n",
    "\n",
    "        env_bounds = 300.0  # 300 m / 300 m / 300 m\n",
    "        self.lower_bounds = np.array([-env_bounds / 2, -env_bounds / 2, 0])\n",
    "        self.upper_bounds = np.array([env_bounds / 2, env_bounds / 2, env_bounds])\n",
    "\n",
    "        self.reset(None)\n",
    "\n",
    "    def reset(self, pos):\n",
    "        if pos is not None:\n",
    "            self.init_pose = np.array(pos)\n",
    "            \n",
    "        self.time = 0.0\n",
    "        self.pose = np.array([0.0, 0.0, 10.0, 0.0, 0.0, 0.0]) if self.init_pose is None else self.init_pose\n",
    "        self.v = np.array([0.0, 0.0, 0.0]) if self.init_velocities is None else self.init_velocities\n",
    "        self.angular_v = np.array([0.0, 0.0, 0.0]) if self.init_angle_velocities is None else self.init_angle_velocities\n",
    "        self.linear_accel = np.array([0.0, 0.0, 0.0])\n",
    "        self.angular_accels = np.array([0.0, 0.0, 0.0])\n",
    "        self.prop_wind_speed = np.array([0., 0., 0., 0.])\n",
    "        self.done = False\n",
    "\n",
    "    def find_body_velocity(self):\n",
    "        body_velocity = np.matmul(earth_to_body_frame(*list(self.pose[3:])), self.v)\n",
    "        return body_velocity\n",
    "\n",
    "    def get_linear_drag(self):\n",
    "        linear_drag = 0.5 * self.rho * self.find_body_velocity()**2 * self.areas * self.C_d\n",
    "        return linear_drag\n",
    "\n",
    "    def get_linear_forces(self, thrusts):\n",
    "        # Gravity\n",
    "        gravity_force = self.mass * self.gravity * np.array([0, 0, 1])\n",
    "        # Thrust\n",
    "        thrust_body_force = np.array([0, 0, sum(thrusts)])\n",
    "        # Drag\n",
    "        drag_body_force = -self.get_linear_drag()\n",
    "        body_forces = thrust_body_force + drag_body_force\n",
    "\n",
    "        linear_forces = np.matmul(body_to_earth_frame(*list(self.pose[3:])), body_forces)\n",
    "        linear_forces += gravity_force\n",
    "        return linear_forces\n",
    "\n",
    "    def get_moments(self, thrusts):\n",
    "        thrust_moment = np.array([(thrusts[3] - thrusts[2]) * self.l_to_rotor,\n",
    "                            (thrusts[1] - thrusts[0]) * self.l_to_rotor,\n",
    "                            0])# (thrusts[2] + thrusts[3] - thrusts[0] - thrusts[1]) * self.T_q])  # Moment from thrust\n",
    "\n",
    "        drag_moment =  self.C_d * 0.5 * self.rho * self.angular_v * np.absolute(self.angular_v) * self.areas * self.dims * self.dims\n",
    "        moments = thrust_moment - drag_moment # + motor_inertia_moment\n",
    "        return moments\n",
    "\n",
    "    def calc_prop_wind_speed(self):\n",
    "        body_velocity = self.find_body_velocity()\n",
    "        phi_dot, theta_dot = self.angular_v[0], self.angular_v[1]\n",
    "        s_0 = np.array([0., 0., theta_dot * self.l_to_rotor])\n",
    "        s_1 = -s_0\n",
    "        s_2 = np.array([0., 0., phi_dot * self.l_to_rotor])\n",
    "        s_3 = -s_2\n",
    "        speeds = [s_0, s_1, s_2, s_3]\n",
    "        for num in range(4):\n",
    "            perpendicular_speed = speeds[num] + body_velocity\n",
    "            self.prop_wind_speed[num] = perpendicular_speed[2]\n",
    "\n",
    "    def get_propeler_thrust(self, rotor_speeds):\n",
    "        '''calculates net thrust (thrust - drag) based on velocity\n",
    "        of propeller and incoming power'''\n",
    "        thrusts = []\n",
    "        for prop_number in range(4):\n",
    "            V = self.prop_wind_speed[prop_number]\n",
    "            D = self.propeller_size\n",
    "            n = rotor_speeds[prop_number]\n",
    "            J = V / n * D\n",
    "            # From http://m-selig.ae.illinois.edu/pubs/BrandtSelig-2011-AIAA-2011-1255-LRN-Propellers.pdf\n",
    "            C_T = max(.12 - .07*max(0, J)-.1*max(0, J)**2, 0)\n",
    "            thrusts.append(C_T * self.rho * n**2 * D**4)\n",
    "        return thrusts\n",
    "\n",
    "    def next_timestep(self, rotor_speeds):\n",
    "        self.calc_prop_wind_speed()\n",
    "        thrusts = self.get_propeler_thrust(rotor_speeds)\n",
    "        self.linear_accel = self.get_linear_forces(thrusts) / self.mass\n",
    "\n",
    "        position = self.pose[:3] + self.v * self.dt + 0.5 * self.linear_accel * self.dt**2\n",
    "        self.v += self.linear_accel * self.dt\n",
    "\n",
    "        moments = self.get_moments(thrusts)\n",
    "\n",
    "        self.angular_accels = moments / self.moments_of_inertia\n",
    "        angles = self.pose[3:] + self.angular_v * self.dt + 0.5 * self.angular_accels * self.angular_accels * self.dt\n",
    "        angles = (angles + 2 * np.pi) % (2 * np.pi)\n",
    "        self.angular_v = self.angular_v + self.angular_accels * self.dt\n",
    "\n",
    "        new_positions = []\n",
    "        for ii in range(3):\n",
    "            if position[ii] <= self.lower_bounds[ii]:\n",
    "                new_positions.append(self.lower_bounds[ii])\n",
    "                self.done = True\n",
    "            elif position[ii] > self.upper_bounds[ii]:\n",
    "                new_positions.append(self.upper_bounds[ii])\n",
    "                self.done = True\n",
    "            else:\n",
    "                new_positions.append(position[ii])\n",
    "\n",
    "        self.pose = np.array(new_positions + list(angles))\n",
    "        self.time += self.dt\n",
    "        if self.time > self.runtime:\n",
    "            self.done = True\n",
    "        return self.done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor / Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipdb import set_trace as debug\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, nb_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.softmax = nn.Softmax()\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1+nb_actions, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        x, a = xs\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # debug()\n",
    "        out = self.fc2(torch.cat([out,a],1))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FLOAT = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "class ExperienceMemory:\n",
    "    \"\"\"Experience Memory Buffer\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize buffer object.\"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # fifo queue\n",
    "        self.batch_size = batch_size\n",
    "        self.general_feeling = 0.0 # not used yet :)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        self.action_size = np.array(action).shape[0]\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sweet_dreams(self, batch_size=64):\n",
    "        \"\"\"Recall and replay experiences in memory, reordering experiences by positive reward.\"\"\"\n",
    "        for e in sorted(self.memory,key=lambda x: x.reward, reverse=False):\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def sample(self, batch_size=64):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        return random.sample(self.memory, k=self.batch_size)\n",
    "    \n",
    "    def batch_samples(self, batch_size=64):\n",
    "        \"\"\"\"\"\"\n",
    "        exp = self.sample(batch_size=64)\n",
    "        \n",
    "        states = np.vstack([e.state for e in exp if e is not None])\n",
    "        actions = np.array([e.action for e in exp if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n",
    "        \n",
    "        rewards = np.array([e.reward for e in exp if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in exp if e is not None])\n",
    "        dones = np.array([e.done for e in exp if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "    \n",
    "class OUNoise:\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process.\n",
    "    https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.mu\n",
    "        \n",
    "def to_numpy(var):\n",
    "    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n",
    "\n",
    "def to_tensor(ndarray, requires_grad=False, dtype=FLOAT):\n",
    "    return Variable(\n",
    "        torch.from_numpy(ndarray), requires_grad=requires_grad\n",
    "    ).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# from ipdb import set_trace as debug\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__( self, task, hp ):\n",
    "                   \n",
    "        self.task = task\n",
    "        self.nb_states = task.state_size\n",
    "        self.nb_actions = task.action_size\n",
    "        self.action_low = task.action_low\n",
    "        self.action_high = task.action_high\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "        \n",
    "        self.use_cuda = 1 if hp['USE_CUDA'] is True else 0\n",
    "        \n",
    "        if int(hp['SEED']) > 0:\n",
    "            self.seed(hp['SEED'])\n",
    "        \n",
    "        self.buffer_size = hp['EXP_BUFFER_SIZE']\n",
    "        self.batch_size = hp['EXP_BATCH_SIZE']\n",
    "\n",
    "        \n",
    "        # Create Actor and Critic Network\n",
    "        net_cfg = {\n",
    "            'hidden1':hp['HIDDEN1'], \n",
    "            'hidden2':hp['HIDDEN2'], \n",
    "            'init_w':hp['INIT_W']\n",
    "        }\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_optim  = Adam(self.actor.parameters(), lr=hp['ACTOR_LR'])\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_optim  = Adam(self.critic.parameters(), lr=hp['CRITIC_LR'])\n",
    "            \n",
    "        self.hard_copy( self.actor, self.actor_target ) \n",
    "        self.hard_copy( self.critic, self.critic_target )\n",
    "        \n",
    "        # Create experience memory buffer\n",
    "        self.memory = ExperienceMemory(self.buffer_size, self.batch_size)\n",
    "        \n",
    "        # init the process of life ... ..\n",
    "        self.random_process = OUNoise(size=self.nb_actions, theta=hp['OU_THETA'], mu=hp['OU_MU'], sigma=hp['OU_SIGMA'])\n",
    "        self.ou_decay = hp['OU_DECAY']\n",
    "\n",
    "        # Hyper-parameters\n",
    "        #self.batch_size = hp.BATCH_SIZE\n",
    "        self.tau = hp['TAU']\n",
    "        self.discount = hp['DISCOUNT']\n",
    "        #self.depsilon = 1.0 / args.epsilon\n",
    "\n",
    "        # \n",
    "        #self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        # nvidia\n",
    "        if hp['USE_CUDA']: \n",
    "            self.cuda()\n",
    "            \n",
    "    def hard_copy(self, source, target):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_( target_param.data * (1.0 - self.tau) + param.data * self.tau )\n",
    "\n",
    "\n",
    "    def update_policy(self):\n",
    "        \n",
    "        # Get Sample batches\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.batch_samples(self.batch_size)\n",
    "\n",
    "        ###########################################\n",
    "        # Prepare for the target q batch\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.critic_target([\n",
    "                to_tensor(next_state_batch),\n",
    "                self.actor_target(to_tensor(next_state_batch)),\n",
    "            ])\n",
    "        #next_q_values.volatile=False\n",
    "\n",
    "        target_q_batch = to_tensor(reward_batch) + \\\n",
    "            self.discount*to_tensor(terminal_batch.astype(np.float))*next_q_values\n",
    "\n",
    "        ############################################\n",
    "        # Critic update\n",
    "        self.critic.zero_grad()\n",
    "\n",
    "        q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "        \n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        ##############################################\n",
    "        # Actor update\n",
    "        self.actor.zero_grad()\n",
    "\n",
    "        policy_loss = -self.critic([\n",
    "            to_tensor(state_batch),\n",
    "            self.actor(to_tensor(state_batch))\n",
    "        ])\n",
    "\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        ###############################################\n",
    "        # Target update\n",
    "        self.soft_update( self.actor, self.actor_target )\n",
    "        self.soft_update( self.critic, self.critic_target )\n",
    "\n",
    "        \n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.actor_target.eval()\n",
    "        self.critic.eval()\n",
    "        self.critic_target.eval()\n",
    "        self.is_training = False\n",
    "\n",
    "        \n",
    "    def cuda(self):\n",
    "        self.use_cuda = 1\n",
    "        self.actor.cuda()\n",
    "        self.actor_target.cuda()\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "        \n",
    "        \n",
    "    def step(self, action, reward, next_state, done):\n",
    "         # Save experience / reward\n",
    "        self.a_t = action\n",
    "        self.observe( reward, next_state, done )\n",
    "\n",
    "        # If we got our minibatch of experience memories..\n",
    "        # learn from them and slowly change belief :)\n",
    "        if len(self.memory) > self.batch_size:           \n",
    "            self.update_policy()\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            self.memory.add(self.s_t, self.a_t, r_t, s_t1, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1.,1.,self.nb_actions)\n",
    "        self.a_t = action\n",
    "        return action\n",
    "    \n",
    "    def act(self, s_t, i_eposode, decay_epsilon=True):\n",
    "        \n",
    "        action = to_numpy(\n",
    "            self.actor(to_tensor(np.array([s_t])))\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        action = (action * self.action_range) + self.action_low\n",
    "        \n",
    "        if(self.ou_decay != 0):\n",
    "            decay = 1 - (i_episode*self.ou_decay)\n",
    "            action += self.is_training*decay*self.random_process.sample()\n",
    "        \n",
    "        self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def reset(self, obs):\n",
    "        self.s_t = self.task.reset(obs)\n",
    "        self.random_process.reset()\n",
    "        return self.s_t\n",
    "\n",
    "    def load_weights(self, output):\n",
    "        if output is None: return\n",
    "\n",
    "        self.actor.load_state_dict(\n",
    "            torch.load('{}/actor.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "        self.critic.load_state_dict(\n",
    "            torch.load('{}/critic.pkl'.format(output))\n",
    "        )\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        torch.save(\n",
    "            self.actor.state_dict(),\n",
    "            '{}/actor.pkl'.format(output)\n",
    "        )\n",
    "        torch.save(\n",
    "            self.critic.state_dict(),\n",
    "            '{}/critic.pkl'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,s):\n",
    "        torch.manual_seed(s)\n",
    "        if self.use_cuda:\n",
    "            torch.cuda.manual_seed(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from physics_sim import PhysicsSim\n",
    "\n",
    "class Hover():\n",
    "    \"\"\"Task (environment) that defines the goal and provides feedback to the agent.\"\"\"\n",
    "    def __init__(self, init_pose=None, init_velocities=None, \n",
    "        init_angle_velocities=None, runtime=5., target_pos=None):\n",
    "        \"\"\"Initialize a Task object.\n",
    "        Params\n",
    "        ======\n",
    "            init_pose: initial position of the quadcopter in (x,y,z) dimensions and the Euler angles\n",
    "            init_velocities: initial velocity of the quadcopter in (x,y,z) dimensions\n",
    "            init_angle_velocities: initial radians/second for each of the three Euler angles\n",
    "            runtime: time limit for each episode\n",
    "            target_pos: target/goal (x,y,z) position for the agent\n",
    "        \"\"\"\n",
    "        # Simulation\n",
    "        self.sim = PhysicsSim(init_pose, init_velocities, init_angle_velocities, runtime) \n",
    "        self.action_repeat = 3\n",
    "\n",
    "        self.state_size = self.action_repeat * (len(self.sim.pose))\n",
    "        self.action_low = 0\n",
    "        self.action_high = 900\n",
    "        self.action_size = 4\n",
    "        self.prev_distance = None\n",
    "\n",
    "        # Goal\n",
    "        self.target_pos = target_pos if target_pos is not None else np.array([0., 0., 10.]) \n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Uses current pose of sim to return reward.\"\"\"\n",
    "        #print(\"POSITION:\\n  {:3.4f},{:3.4f},{:3.4f}\".format(self.sim.pose[0],self.sim.pose[1],self.sim.pose[2]))\n",
    "        sum_distance = (abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "        # D = √[(x₂ - x₁)² + (y₂ - y₁)² +(z₂ - z₁)²]\n",
    "        distance = np.linalg.norm(self.sim.pose[:3] - self.target_pos)\n",
    "        #x0,y0,z0 = self.target_pos[:3]\n",
    "        #x1,y2,z1 = self.sim.pose[:3] \n",
    "    \n",
    "        reward = 1.- .003*sum_distance\n",
    "        if sum_distance < 0.4:\n",
    "            return 100\n",
    "        #reward = (10.+self.sim.time) - ( distance * (abs(self.sim.v)).sum() )\n",
    "        #max_xy_angle = max(abs(self.sim.pose[3]),abs(self.sim.pose[4]))\n",
    "        \n",
    "        # we want to \"stay\" ..so we use every signal that we are not staying as sum penalty\n",
    "        #sum_distance = (abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "        #sum_angle = abs(self.sim.pose[3:]).sum()\n",
    "        #sum_velocity = abs(self.sim.v).sum()\n",
    "        #penalty = (sum_distance+(sum_velocity*0.01)) # + sum_velocity + sum_angle)\n",
    "        #reward = (1+self.sim.time) - (distance + (angle / 4))\n",
    "        #reward = 25 - penalty\n",
    "\n",
    "        #reward = 10 - distance # + (sum_angle*0.025)\n",
    "        #reward = reward * 0.1\n",
    "        \n",
    "        #punishment = -( distance + sum_angle + sum_velocity)\n",
    "        \n",
    "        #reward  = 10. - (distance * 0.1)\n",
    "        #if self.prev_distance == None or self.prev_distance == distance:\n",
    "        #    reward = 10.0 - (distance * 0.1)\n",
    "        #elif self.prev_distance > distance:\n",
    "        #    reward = 20.0 - (distance * 0.1)\n",
    "        #elif self.prev_distance < distance:\n",
    "        #    reward = -20.0 - (distance * 0.1)\n",
    "            \n",
    "        self.prev_distance = distance\n",
    "        return reward\n",
    "\n",
    "    def step(self, rotor_speeds):\n",
    "        \"\"\"Uses action to obtain next state, reward, done.\"\"\"\n",
    "        reward = 0\n",
    "        pose_all = []\n",
    "        done = False\n",
    "        #print(\"Taking repeated steps:\")\n",
    "        for _ in range(self.action_repeat):\n",
    "            done = self.sim.next_timestep(rotor_speeds) # update the sim pose and velocities\n",
    "            reward += self.get_reward()\n",
    "            if reward == 100:\n",
    "                done = 1\n",
    "            #if reward <= -3.0:\n",
    "            #    done = True\n",
    "            #if done and self.sim.time < self.sim.runtime:\n",
    "            #    reward = -100\n",
    "\n",
    "            #print(self.sim.pose)\n",
    "            #distances = np.array(self.sim.pose[:3] - self.target_pos)\n",
    "            #print(distances)\n",
    "            #print(np.concatenate([self.sim.pose,distances]))\n",
    "            #pose_all.append(np.concatenate([self.sim.pose,distances]))\n",
    "            pose_all.append(np.concatenate([self.sim.pose]))\n",
    "        next_state = np.concatenate(pose_all)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self, pos):\n",
    "        \"\"\"Reset the sim to start a new episode.\"\"\"\n",
    "        self.sim.reset(pos)\n",
    "        distances = np.array(self.sim.pose[:3] - self.target_pos)\n",
    "        state = np.concatenate([self.sim.pose]) \n",
    "        state = np.concatenate([state] * self.action_repeat) \n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "hparams = {\n",
    "    'EPISODES'       : 2000,\n",
    "    'EP_MAX_RUNTIME' : 20,\n",
    "    'SEED'           : 0,\n",
    "    'USE_CUDA'       : False,\n",
    "    # deep nn\n",
    "    'ACTOR_LR'    : 0.0001, # adjustment per backprop\n",
    "    'CRITIC_LR'   : 0.001, # adjustment per backprop\n",
    "    'INIT_W'      : 0.003, # init weight distribution size\n",
    "    'HIDDEN1'     : 16, \n",
    "    'HIDDEN2'     : 16,\n",
    "#    'ACTOR_L2_RR' : 0.01, # L2/ridge regularization\n",
    "#    'CRITIC_L2_RR': 0.01, # L2/ridge regularization\n",
    "    'TAU'         : 0.001, # The factor of convergence for target nets\n",
    "    'DROPOUT'     : 0.25, # Neuron dropout-factor (not implemented atm)\n",
    "    \n",
    "    # Ornstein-Uhlenbeck Exploration\n",
    "    'OU_MU'    : 0.0,   # noise overall offset\n",
    "    'OU_THETA' : .15,  # how “fast” the variable reverts towards to the mean\n",
    "    'OU_SIGMA' : .3, # scale factor\n",
    "    'OU_DECAY' : 1.5, # zero to disable OU\n",
    "    \n",
    "    # Q-learning\n",
    "    'DISCOUNT' : .99,  # value discount factor\n",
    "\n",
    "    # Instead of pixels we use a batch of environmental data / states \n",
    "    # (experiences), that we feed to our deep neural networks\n",
    "    'EXP_BUFFER_SIZE' : 8000000,\n",
    "    'EXP_BATCH_SIZE' : 100 # \n",
    "}\n",
    "# create a decay\n",
    "if hparams['OU_DECAY'] != 0:\n",
    "    hparams['OU_DECAY'] = 1 / ( hparams['EPISODES'] * hparams['OU_DECAY'] )\n",
    "\n",
    "#hparams = namedtuple(\"HyperParams\",hparams)\n",
    "\n",
    "init_pose = np.array([.5, .5, 99.5, 0., 0., 0.])\n",
    "target_pos = np.array([0., 0., 100.])\n",
    "#task = Task(runtime=2.5, target_pos=target_pos)\n",
    "task = Hover(runtime=hparams['EP_MAX_RUNTIME'], init_pose=init_pose, target_pos=target_pos)\n",
    "agent = Agent(task, hparams) \n",
    "print(agent.actor)\n",
    "print(agent.critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run age..*cough*  Fly Agent ..Fly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "num_episodes = hparams['EPISODES']\n",
    "\n",
    "worst_score = 0\n",
    "best_score = 0\n",
    "\n",
    "reward_labels = ['episode', 'reward']\n",
    "reward_results = {x : [] for x in reward_labels}\n",
    "\n",
    "starting_positions = [\n",
    "    [0., 0, 95.],\n",
    "    [0., 0, 105.]\n",
    "]\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    ep_score = 0\n",
    "    #x = np.random.uniform(-2.,2.)\n",
    "    #y = np.random.uniform(-2.,2.)\n",
    "    #z = np.random.uniform(98.,102.)\n",
    "    #i = random.sample(starting_positions,len(starting_positions)-1)\n",
    "    pos = starting_positions[0]#i_episode%2]\n",
    "    state = agent.reset([pos[0], pos[1], pos[2], 0., 0., 0.])\n",
    "    #state = agent.reset_episode(starting_positions[i_episode%2])\n",
    "\n",
    "    pose_bucket = {'x':[],'y':[],'z':[]}\n",
    "    rotor = {'time':[],'1':[],'2':[],'3':[],'4':[]}\n",
    "    rewards = []\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, i_episode) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        ep_score += reward\n",
    "        best_score = max(best_score , ep_score)\n",
    "        worst_score = min(worst_score , ep_score)\n",
    "        x,y,z = task.sim.pose[:3]\n",
    "        pose_bucket['x'].append(x)\n",
    "        pose_bucket['y'].append(y)\n",
    "        pose_bucket['z'].append(z)\n",
    "        rewards.append(reward)\n",
    "        rotor['time'].append(task.sim.time)\n",
    "        rotor['1'].append(action[0])\n",
    "        rotor['2'].append(action[1])\n",
    "        rotor['3'].append(action[2])\n",
    "        rotor['4'].append(action[3])\n",
    "        \n",
    "        \n",
    "        if done or reward <= -1000:\n",
    "            #clear_output(wait=True)\n",
    "            f = plt.figure(figsize=(15,6))\n",
    "            ax1 = f.add_subplot(2, 2, 1)\n",
    "            #plt.tight_layout()\n",
    "            ax1.plot(rewards)\n",
    "            #ax = fig.gca(projection='3d')\n",
    "            ax2 = f.add_subplot(2,2,2,projection='3d')\n",
    "            ax2.plot(pose_bucket['x'], pose_bucket['y'], pose_bucket['z'], label='quadraview')\n",
    "            ax2.set_zlim3d(0, 300)\n",
    "            ax2.set_xlim(-300, 300)\n",
    "            ax2.set_ylim(-300, 300)\n",
    "            \n",
    "            ax3 = f.add_subplot(2,2,3)\n",
    "            ax3.plot(rotor['time'], rotor['1'], label='r1')\n",
    "            ax3.plot(rotor['time'], rotor['2'], label='r2')\n",
    "            ax3.plot(rotor['time'], rotor['3'], label='r3')\n",
    "            ax3.plot(rotor['time'], rotor['4'], label='r4')\n",
    "            \n",
    "            #ax.quiver(pose_bucket['x'], pose_bucket['y'], pose_bucket['z'], 0.0, 0.0, 0.0, label='quadraview')\n",
    "            plt.show()\n",
    "            avg_score = ep_score / len(rewards)\n",
    "            print(\"Episode = {:4d}, score = {:7.3f} (best = {:7.3f} , worst = {:7.3f})\".format(\n",
    "               i_episode, avg_score, ep_score, best_score, worst_score), end=\"\\n\")\n",
    "            break\n",
    "    reward_results['episode'].append(i_episode)\n",
    "    reward_results['reward'].append(ep_score/len(rewards))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(reward_results['episode'], reward_results['reward'], label='avg reward / episode')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "runtime = 150.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 100., 0., 0., 0.])  # initial pose\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "target_pos = np.array([0., 0., 100.])   \n",
    "\n",
    "\n",
    "task = Hover(init_pose, init_velocities, init_angle_velocities, runtime, target_pos)\n",
    "#task = Hover( init_pose, target_pos, runtime=runtime, target_pos=target_pos )\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "\n",
    "agent.eval()\n",
    "state = agent.reset(None)\n",
    "total_reward = 0\n",
    "while True:\n",
    "    rotor_speeds = agent.act(state, 1)\n",
    "    print(rotor_speeds)\n",
    "    next_state, reward, done = task.step(rotor_speeds)\n",
    "    to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "    for ii in range(len(labels)):\n",
    "        results[labels[ii]].append(to_write[ii])\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(\"Total episode reward : {}\".format(total_reward))\n",
    "        total_reward = 0\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['rotor_speed1'], label='r1')\n",
    "plt.plot(results['time'], results['rotor_speed2'], label='r2')\n",
    "plt.plot(results['time'], results['rotor_speed3'], label='r3')\n",
    "plt.plot(results['time'], results['rotor_speed4'], label='r4')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['phi'], label='phi')\n",
    "plt.plot(results['time'], results['theta'], label='theta')\n",
    "plt.plot(results['time'], results['psi'], label='psi')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['x'], label='x')\n",
    "plt.plot(results['time'], results['y'], label='y')\n",
    "plt.plot(results['time'], results['z'], label='z')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
